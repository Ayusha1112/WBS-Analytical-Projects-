# -*- coding: utf-8 -*-
"""TA_5590002

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17OL9e4FYZzP1b2wehQQy9PwByXJ5OG4X

**Retrieval Augmented Generation (RAG) System for Indian Cooking Recipes**

By: Ayusha Jadhav
ID: 5590002

Original Dataset Location:        https://www.kaggle.com/datasets/kanishk307/6000-indian-food-recipes-dataset/data


The Retrieval Augmented Generation (RAG) System for Indian Cooking Recipes is a project that aims to enhance the process of generating new recipes based on user queries.

This system uses cutting-edge natural language processing techniques, including sentence embedding and a fine-tuned GPT-2 language model, to create personalized and relevant recipe suggestions. By integrating a vector database for efficient retrieval of related recipe information, the RAG System is able to provide users with unique and tailored culinary experiences specially with Indian Cuisine.

Step 1: Install/Import Important Packages
"""

!pip install sentence-transformers transformers faiss-cpu pandas openpyxl accelerate
!pip install datasets

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import json
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import load_dataset

"""Step 2: Chunking, Embedding, and Fine- Tuning (GPT2)"""

# Load the dataset
df = pd.read_excel('/content/Copy of IndianFoodDatasetXLS.xlsx')

# Load the embedding model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Chunk the ingredients and storing the other metadata
chunks = []
metadata = []
chunk_size = 512

for index, row in df.iterrows():
    instructions = row['Instructions']
    # Check if 'instructions' is a string
    if isinstance(instructions, str):
        for i in range(0, len(instructions), chunk_size):
            chunk = instructions[i:i+chunk_size]
            chunks.append(chunk)
            metadata.append({
                'name': row['Name'],
                'ingredients': row['Ingredients']
            })

# Embed all chunks and storing the embeddings in a vector database
embeddings = embedding_model.encode(chunks, convert_to_tensor=True)


index = faiss.IndexFlatL2(embeddings.shape[1])
faiss.normalize_L2(embeddings.numpy())
index.add(embeddings.numpy())
faiss.write_index(index, '/content/food_recipes_index.faiss')

with open('/content/food_recipes_chunks.json', 'w') as f:
    json.dump(chunks, f)

with open('/content/food_recipes_metadata.json', 'w') as f:
    json.dump(metadata, f)

# Fine-tuning GPT-2 model
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
gpt_model = GPT2LMHeadModel.from_pretrained(model_name)

# Set the pad_token to eos_token
tokenizer.pad_token = tokenizer.eos_token

# Create a dataset for fine-tuning
instructions = df['Instructions'].dropna().tolist()
train_texts, val_texts = train_test_split(instructions, test_size=0.1)

def create_text_file(texts, file_path):
    with open(file_path, 'w') as f:
        for text in texts:
            f.write(text + '\n')

create_text_file(train_texts, 'train.txt')
create_text_file(val_texts, 'val.txt')

def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)

# Load and tokenize the datasets
train_dataset = load_dataset('text', data_files={'train': 'train.txt'})['train']
val_dataset = load_dataset('text', data_files={'validation': 'val.txt'})['validation']

train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])
val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

training_args = TrainingArguments(
    output_dir='./results',
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    save_steps=10_000,
    save_total_limit=2,
    logging_steps=500,
    evaluation_strategy="steps",
    eval_steps=500
)

trainer = Trainer(
    model=gpt_model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()
trainer.save_model('./fine_tuned_gpt2')

# Load the fine-tuned model
gpt_model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')

"""Step 3: Developing an appropriate Prompt Template and Define test queries"""

import torch

def create_prompt(query, retrieved_metadata):
    prompt = (
        f"Query: {query}\n\n"
        f"Name: {retrieved_metadata['name']}\n"
        f"Ingredients: {retrieved_metadata['ingredients']}\n"
        f"Instructions:\n"
       )
    return prompt

def query_pipeline(query):
    try:
        # Embed the query
        query_embedding = embedding_model.encode([query], convert_to_tensor=True)
        faiss.normalize_L2(query_embedding.numpy())

        # Retrieve the most relevant chunk
        D, I = index.search(query_embedding.numpy(), k=1)
        retrieved_metadata = metadata[I[0][0]]

        # Create the prompt
        prompt = create_prompt(query, retrieved_metadata)

        # Tokenize the prompt
        inputs = tokenizer(prompt, return_tensors='pt', padding=True)
        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']

        # Generate the response
        outputs = gpt_model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=512,
            num_return_sequences=1,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            repetition_penalty=2.0,
            pad_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return response
    except Exception as e:
        return f"An error occurred: {e}"

# Defining Test Queries
test_queries = [
    "Generate a new recipe using chicken and rice.",
    "Create a vegetarian recipe with pasta.",
    "Write a new recipe for a spicy Mexican dish.",
    "Generate a dessert recipe that's gluten-free.",
    "Create a new vegan breakfast recipe."
]

for query in test_queries:
    response = query_pipeline(query)
    print(f"Query: {query}\nResponse: {response}\n")